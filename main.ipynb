{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_pre_processing_and_transform.create_torch_datset import TorchDataModule, TorchDatasetTS\n",
    "from data_pre_processing_and_transform.data_preprocessing import DataPreprocessing\n",
    "\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\n",
    "tqdm.pandas() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "\n",
    "    def __init__(self, n_features, sequence_length, number_of_dependent_signals, n_hidden=64, n_layers = 2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_hidden = n_hidden\n",
    "        self.sequence_length = sequence_length\n",
    "        self.number_of_dependent_signals = number_of_dependent_signals\n",
    "\n",
    "        \"\"\"self.lstm = nn.LSTM(\n",
    "            input_size=n_features,\n",
    "            hidden_size=n_hidden,\n",
    "            batch_first=True,\n",
    "            num_layers=  n_layers,\n",
    "            dropout=0.2)\n",
    "            \"\"\"\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(input_size = n_features,\n",
    "                            hidden_size = n_hidden,\n",
    "                            num_layers=n_layers,\n",
    "                            batch_first=True,\n",
    "                            dropout=0.2)\n",
    "        \n",
    "        #self.bn1 = nn.BatchNorm1d()\n",
    "\n",
    "\n",
    "\n",
    "        self.regressor1 = nn.Linear(n_hidden, sequence_length * number_of_dependent_signals)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #self.lstm.flatten_prameters()  # Even if we don't use flatten prameter the code will work. But it helps in distributed training of GPU\n",
    "\n",
    "        #out, (hidden, _) = self.lstm(x)\n",
    "        #out = hidden[-1]\n",
    "\n",
    "        out, (_, _) = self.lstm1(x)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        return self.regressor1(out)\n",
    "    \n",
    "\n",
    "class Predictor(pl.LightningModule):\n",
    "    def __init__(self, n_features, sequence_length, number_of_dependent_signals):\n",
    "        super().__init__()\n",
    "        self.model = LSTMModel(n_features, sequence_length, number_of_dependent_signals)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x, labels =None):\n",
    "        output = self.model(x)\n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(output, labels.unsqueeze(dim=1))\n",
    "        return loss, output   \n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        sequences = batch['sequence']\n",
    "        labels = batch['label']\n",
    "\n",
    "        loss, output = self(sequences, labels)\n",
    "        self.log('train_loss', loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        sequences = batch['sequence']\n",
    "        labels = batch['label']\n",
    "\n",
    "        loss, output = self(sequences, labels) #\n",
    "        self.log('val_loss', loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        sequences = batch['sequence']\n",
    "        labels = batch['label']\n",
    "\n",
    "        loss, output = self(sequences, labels)\n",
    "        self.log('test_loss', loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return optim.AdamW(self.parameters(), lr = 0.0001)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/resampled_20180810150607.csv\n",
      "datasets/resampled_20190401121727.csv\n",
      "datasets/resampled_20190401145936.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"for item in data_module.train_dataloader(): \\n    print(item['sequence'].shape)\\n    print(item['label'].shape)\\n    break\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geo_data = ['latitude_degree','longitude_degree']\n",
    "\n",
    "independent_signals = ['accelerator_pedal','accelerator_pedal_gradient_sign','brake_pressure','steering_angle_calculated','steering_angle_calculated_sign']\n",
    "dependent_signals = ['vehicle_speed','roll_angle','pitch_angle']\n",
    "\n",
    "sequence_length = 10\n",
    "\n",
    "N_EPOCHS = 100\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "\n",
    "data_prep = DataPreprocessing('datasets/', independent_signals, dependent_signals, sequence_length,train_size=.75)\n",
    "data_prep.fit_transform()\n",
    "\n",
    "train_sequence = data_prep.get_sequences_train\n",
    "test_sequence = data_prep.get_sequences_test\n",
    "\n",
    "data_module = TorchDataModule(train_sequence, test_sequence, batch_size = BATCH_SIZE)\n",
    "data_module.setup()\n",
    "\n",
    "\"\"\"for item in data_module.train_dataloader(): \n",
    "    print(item['sequence'].shape)\n",
    "    print(item['label'].shape)\n",
    "    break\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 1250), started 0:00:16 ago. (Use '!kill 1250' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-2c51f521d05aaa57\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-2c51f521d05aaa57\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints\",\n",
    "    filename=\"best-checkpoint\",\n",
    "    save_top_k=1, \n",
    "    verbose=True, \n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\")\n",
    "\n",
    "logger = TensorBoardLogger(\"logs\", name='LSTM')\n",
    "\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss',\n",
    "                                        patience=25)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    logger=logger,\n",
    "    callbacks=[early_stopping_callback, checkpoint_callback],\n",
    "    max_epochs=N_EPOCHS\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Predictor(n_features=len(independent_signals),\n",
    "                   sequence_length=sequence_length,\n",
    "                   number_of_dependent_signals=len(dependent_signals))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m      2\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mfit(model, data_module\u001b[38;5;241m.\u001b[39mtrain_dataloader(), data_module\u001b[38;5;241m.\u001b[39mval_dataloader())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"for item in data_module.train_dataloader(): \\n    print(item['sequence'].shape)\\n    print(item['label'].shape)\\n    print('------')\\n    break\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"for item in data_module.train_dataloader(): \n",
    "    print(item['sequence'].shape)\n",
    "    print(item['label'].shape)\n",
    "    print('------')\n",
    "    break\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
