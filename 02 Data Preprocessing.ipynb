{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\n",
    "tqdm.pandas() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_data = ['latitude_degree','longitude_degree']\n",
    "\n",
    "independent_signals = ['accelerator_pedal','accelerator_pedal_gradient_sign','brake_pressure','steering_angle_calculated','steering_angle_calculated_sign']\n",
    "dependent_signals = ['vehicle_speed','roll_angle','pitch_angle']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spliting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessing():\n",
    "    def __init__(self, directory, independent_signals, dependent_signals, sequence_length, train_size=.9, do_train_test_split = True):\n",
    "        \n",
    "      self.directory = directory\n",
    "      self.do_train_test_split = do_train_test_split\n",
    "      self.list_of_path = DataPreprocessing.list_csv_files(directory)\n",
    "      self.train_size = train_size\n",
    "      self.independent_signals = independent_signals\n",
    "      self.dependent_signals = dependent_signals\n",
    "      self.sequence_length = sequence_length\n",
    "\n",
    "      self.train_df_batch = []\n",
    "      self.test_df_batch = []\n",
    "      self.dataset_batch = []\n",
    "      self.train_sequences = []\n",
    "      self.test_sequences = []\n",
    "\n",
    "      self.scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "    \n",
    "    @staticmethod\n",
    "    def split_dataset(data, train_size):\n",
    "      train_size = int(len(data)* train_size)\n",
    "      train_df, test_df = data[:train_size], data[train_size + 1:]\n",
    "      return train_df, test_df  \n",
    "\n",
    "    @staticmethod\n",
    "    def list_csv_files(directory):\n",
    "      csv_files = []\n",
    "      for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".csv\"):\n",
    "          csv_files.append(filename)\n",
    "      return csv_files    \n",
    "    \n",
    "    @staticmethod\n",
    "    def load_data_from_a_specific_path(file_path):\n",
    "      dataset = pd.read_csv(file_path)\n",
    "      dataset.timestamp = pd.to_datetime(dataset.timestamp)\n",
    "      dataset = dataset.set_index('timestamp')\n",
    "      return dataset\n",
    "    \n",
    "    \n",
    "    def scaling_fit(self):\n",
    "      self.scaler.fit(pd.concat(self.train_df_batch))\n",
    "\n",
    "    def scaling_transform(self, dataset):\n",
    "      return pd.DataFrame(self.scaler.transform(dataset),\n",
    "                          index=dataset.index,\n",
    "                          columns=dataset.columns)\n",
    "    \n",
    "    def data_loader(self, file_path):\n",
    "\n",
    "      path = '{}{}'.format(self.directory, file_path) \n",
    "      print(path)\n",
    "      dataset = pd.read_csv(path)\n",
    "      dataset.timestamp = pd.to_datetime(dataset.timestamp)\n",
    "      dataset = dataset.set_index('timestamp')\n",
    "      return dataset\n",
    "    \n",
    "    def fit_transform(self):  \n",
    "      for each_file_path in self.list_of_path:\n",
    "        # Loading data from each file\n",
    "        dataset = self.data_loader(each_file_path)\n",
    "\n",
    "        # Calculate min and max value of all features this will be used for scaling\n",
    "        self.dataset_batch.append(dataset)\n",
    "        \n",
    "        train_df, test_df = DataPreprocessing.split_dataset(dataset, self.train_size)\n",
    "        self.train_df_batch.append(train_df)\n",
    "        self.test_df_batch.append(test_df)\n",
    "\n",
    "      # Scaling\n",
    "      self.scaling_fit()\n",
    "      \n",
    "      # Scaling transform\n",
    "      for batch_idx in range(len(self.train_df_batch)):\n",
    "        self.train_df_batch[batch_idx] = (self.scaling_transform(self.train_df_batch[batch_idx]))\n",
    "        self.test_df_batch[batch_idx] = self.scaling_transform(self.test_df_batch[batch_idx])\n",
    "\n",
    "\n",
    "      # For train data\n",
    "      self.create_sequences(self.sequence_length)\n",
    "      # For test data\n",
    "      self.create_sequences(self.sequence_length, get_sequences_for_train = False)\n",
    "\n",
    "\n",
    "    def transform(self, path):\n",
    "        dataset = DataPreprocessing.load_data_from_a_specific_path(path)\n",
    "        self.datset_sequences = []\n",
    "        scaled_dataset = (self.scaling_transform(dataset))\n",
    "        self.create_sequences_for_a_batch(scaled_dataset, self.sequence_length, None)\n",
    "        return self.datset_sequences\n",
    "\n",
    "\n",
    "    def create_sequences_for_a_batch(self, input_data, sequence_length, get_sequences_for_train):\n",
    "\n",
    "      data_size = len(input_data)\n",
    "\n",
    "      for i in range(data_size - sequence_length):\n",
    "\n",
    "        sequence = input_data[i: i+sequence_length][self.independent_signals]\n",
    "\n",
    "        if get_sequences_for_train: \n",
    "          label = input_data[i: i + sequence_length][self.dependent_signals]\n",
    "          self.train_sequences.append((sequence, label))\n",
    "\n",
    "        elif get_sequences_for_train ==None:\n",
    "          self.datset_sequences.append((sequence, None)) \n",
    "\n",
    "        else:\n",
    "          label = input_data[i: i + sequence_length][self.dependent_signals]\n",
    "          self.test_sequences.append((sequence,label))  \n",
    "\n",
    "\n",
    "    def create_sequences(self, sequence_length, get_sequences_for_train=True):\n",
    "\n",
    "      if get_sequences_for_train:\n",
    "\n",
    "        for each_batch in self.train_df_batch:\n",
    "          self.create_sequences_for_a_batch(each_batch, sequence_length, get_sequences_for_train)\n",
    "\n",
    "      else:\n",
    "\n",
    "        for each_batch in self.test_df_batch:\n",
    "          self.create_sequences_for_a_batch(each_batch, sequence_length, get_sequences_for_train)  \n",
    "        \n",
    "\n",
    "    # Getter method\n",
    "    @property\n",
    "    def get_train_df(self):\n",
    "      return self.train_df_batch\n",
    "\n",
    "    @property\n",
    "    def get_test_df(self):\n",
    "      return self.test_df_batch \n",
    "    \n",
    "    @property\n",
    "    def get_sequences_train(self):\n",
    "      return self.train_sequences\n",
    "    \n",
    "    @property\n",
    "    def get_sequences_test(self):\n",
    "      return self.test_sequences  \n",
    "    \n",
    "\n",
    "class TorchDatasetTS(Dataset):\n",
    "\n",
    "  def __init__(self, sequences):\n",
    "    #super().__init__()\n",
    "    self.sequences = sequences\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.sequences)\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    sequence, label = self.sequences[idx]\n",
    "    \n",
    "    return dict(\n",
    "      sequence= torch.Tensor(sequence.to_numpy()),\n",
    "      label = torch.Tensor(label.to_numpy()).reshape(-1,1)\n",
    "    )\n",
    "  \n",
    "\n",
    "class TorchDataModule(pl.LightningDataModule):\n",
    "\n",
    "  def __init__(self, train_sequences, test_sequences, batch_size):\n",
    "\n",
    "    super().__init__()\n",
    "    self.train_sequences = train_sequences\n",
    "    self.test_sequences = test_sequences\n",
    "    self.batch_size = batch_size\n",
    "\n",
    "  def setup(self):\n",
    "    self.train_datset = TorchDatasetTS(self.train_sequences)  \n",
    "    self.test_datset = TorchDatasetTS(self.test_sequences)  \n",
    "\n",
    "  def train_dataloader(self):\n",
    "    return DataLoader(\n",
    "      self.train_datset,\n",
    "      batch_size = self.batch_size,\n",
    "      shuffle = False\n",
    "    )  \n",
    "  \n",
    "  def val_dataloader(self):   # TODO make seprate data for validation\n",
    "    return DataLoader(\n",
    "      self.test_datset,\n",
    "      batch_size = 1,\n",
    "      shuffle = False\n",
    "    )  \n",
    "  \n",
    "  def test_dataloader(self):\n",
    "    return DataLoader(\n",
    "      self.test_datset,\n",
    "      batch_size = 1,\n",
    "      shuffle = False\n",
    "    )  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/resampled_20180810150607.csv\n",
      "datasets/resampled_20190401121727.csv\n",
      "datasets/resampled_20190401145936.csv\n"
     ]
    }
   ],
   "source": [
    "sequence_length = 10\n",
    "data_prep = DataPreprocessing('datasets/', independent_signals, dependent_signals, sequence_length)\n",
    "data_prep.fit_transform()\n",
    "\n",
    "train_sequence = data_prep.get_sequences_train\n",
    "test_sequence = data_prep.get_sequences_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accelerator_pedal</th>\n",
       "      <th>accelerator_pedal_gradient_sign</th>\n",
       "      <th>brake_pressure</th>\n",
       "      <th>steering_angle_calculated</th>\n",
       "      <th>steering_angle_calculated_sign</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-08-10 13:06:54</th>\n",
       "      <td>0.536650</td>\n",
       "      <td>-0.391304</td>\n",
       "      <td>-0.996302</td>\n",
       "      <td>-0.874614</td>\n",
       "      <td>-1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-10 13:06:55</th>\n",
       "      <td>0.512533</td>\n",
       "      <td>-0.920000</td>\n",
       "      <td>-0.997506</td>\n",
       "      <td>-0.958218</td>\n",
       "      <td>-1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-10 13:06:56</th>\n",
       "      <td>0.210929</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>-0.996674</td>\n",
       "      <td>-0.954475</td>\n",
       "      <td>-1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-10 13:06:57</th>\n",
       "      <td>-0.400822</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>-0.996683</td>\n",
       "      <td>-0.984006</td>\n",
       "      <td>-0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-10 13:06:58</th>\n",
       "      <td>-0.731603</td>\n",
       "      <td>-0.640000</td>\n",
       "      <td>-0.997173</td>\n",
       "      <td>-0.942241</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-10 13:06:59</th>\n",
       "      <td>-0.774321</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.996591</td>\n",
       "      <td>-0.852417</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-10 13:07:00</th>\n",
       "      <td>-0.774321</td>\n",
       "      <td>-0.960000</td>\n",
       "      <td>-0.996258</td>\n",
       "      <td>-0.916477</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-10 13:07:01</th>\n",
       "      <td>-0.976465</td>\n",
       "      <td>-0.640000</td>\n",
       "      <td>-0.996912</td>\n",
       "      <td>-0.902894</td>\n",
       "      <td>-1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-10 13:07:02</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.996591</td>\n",
       "      <td>-0.928943</td>\n",
       "      <td>-1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-10 13:07:03</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.810177</td>\n",
       "      <td>-0.997826</td>\n",
       "      <td>-1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     accelerator_pedal  accelerator_pedal_gradient_sign  \\\n",
       "timestamp                                                                 \n",
       "2018-08-10 13:06:54           0.536650                        -0.391304   \n",
       "2018-08-10 13:06:55           0.512533                        -0.920000   \n",
       "2018-08-10 13:06:56           0.210929                         0.360000   \n",
       "2018-08-10 13:06:57          -0.400822                         0.400000   \n",
       "2018-08-10 13:06:58          -0.731603                        -0.640000   \n",
       "2018-08-10 13:06:59          -0.774321                        -1.000000   \n",
       "2018-08-10 13:07:00          -0.774321                        -0.960000   \n",
       "2018-08-10 13:07:01          -0.976465                        -0.640000   \n",
       "2018-08-10 13:07:02          -1.000000                        -1.000000   \n",
       "2018-08-10 13:07:03          -1.000000                        -1.000000   \n",
       "\n",
       "                     brake_pressure  steering_angle_calculated  \\\n",
       "timestamp                                                        \n",
       "2018-08-10 13:06:54       -0.996302                  -0.874614   \n",
       "2018-08-10 13:06:55       -0.997506                  -0.958218   \n",
       "2018-08-10 13:06:56       -0.996674                  -0.954475   \n",
       "2018-08-10 13:06:57       -0.996683                  -0.984006   \n",
       "2018-08-10 13:06:58       -0.997173                  -0.942241   \n",
       "2018-08-10 13:06:59       -0.996591                  -0.852417   \n",
       "2018-08-10 13:07:00       -0.996258                  -0.916477   \n",
       "2018-08-10 13:07:01       -0.996912                  -0.902894   \n",
       "2018-08-10 13:07:02       -0.996591                  -0.928943   \n",
       "2018-08-10 13:07:03       -0.810177                  -0.997826   \n",
       "\n",
       "                     steering_angle_calculated_sign  \n",
       "timestamp                                            \n",
       "2018-08-10 13:06:54                           -1.00  \n",
       "2018-08-10 13:06:55                           -1.00  \n",
       "2018-08-10 13:06:56                           -1.00  \n",
       "2018-08-10 13:06:57                           -0.68  \n",
       "2018-08-10 13:06:58                            1.00  \n",
       "2018-08-10 13:06:59                            1.00  \n",
       "2018-08-10 13:07:00                            0.70  \n",
       "2018-08-10 13:07:01                           -1.00  \n",
       "2018-08-10 13:07:02                           -1.00  \n",
       "2018-08-10 13:07:03                           -1.00  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequence[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "\n",
    "    def __init__(self, n_features, sequence_length, number_of_dependent_signals, n_hidden=128, n_layers = 2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_hidden = n_hidden\n",
    "        self.sequence_length = sequence_length\n",
    "        self.number_of_dependent_signals = number_of_dependent_signals\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=n_features,\n",
    "            hidden_size=n_hidden,\n",
    "            batch_first=True,\n",
    "            num_layers=  n_layers,\n",
    "            dropout=0.2\n",
    "            \n",
    "        )\n",
    "\n",
    "        self.regressor = nn.Linear(n_hidden, sequence_length * number_of_dependent_signals)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.lstm.flatten_prameters()  # Even if we don't use flatten prameter the code will work. But it helps in distributed training of GPU\n",
    "\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        out = hidden[-1]\n",
    "\n",
    "        return self.regressor(out)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(pl.LightningModule):\n",
    "    def __init__(self, n_features, sequence_length, number_of_dependent_signals):\n",
    "        super().__init__\n",
    "        self.model = LSTMModel(n_features, sequence_length, number_of_dependent_signals)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x, labels =None):\n",
    "        output = self.model(x)\n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(output, labels.unsqueeze(dim=1))\n",
    "        return loss, output   \n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        sequences = batch['sequence']\n",
    "        labels = batch['label']\n",
    "\n",
    "        loss, output = self(sequences, labels)\n",
    "        self.log('train_loss', loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        sequences = batch['sequence']\n",
    "        labels = batch['label']\n",
    "\n",
    "        loss, output = self(sequences, labels) #\n",
    "        self.log('val_loss', loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        sequences = batch['sequence']\n",
    "        labels = batch['label']\n",
    "\n",
    "        loss, output = self(sequences, labels)\n",
    "        self.log('test_loss', loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return optim.AdamW(self.parameters(), lr = 0.0001)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 100\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "data_module = TorchDataModule(train_sequence, test_sequence, batch_size = BATCH_SIZE)\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10, 5])\n",
      "torch.Size([64, 30, 1])\n"
     ]
    }
   ],
   "source": [
    "for item in data_module.train_dataloader(): \n",
    "    print(item['sequence'].shape)\n",
    "    print(item['label'].shape)\n",
    "    break\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
