{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "tqdm.pandas() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_data = ['latitude_degree','longitude_degree']\n",
    "\n",
    "independent_signals = ['accelerator_pedal','accelerator_pedal_gradient_sign','brake_pressure','steering_angle_calculated','steering_angle_calculated_sign']\n",
    "dependent_signals = ['vehicle_speed','roll_angle','pitch_angle']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spliting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessing():\n",
    "    def __init__(self, directory, independent_signals, dependent_signals, sequence_length, train_size=.9, do_train_test_split = True):\n",
    "        \n",
    "      self.directory = directory\n",
    "      self.do_train_test_split = do_train_test_split\n",
    "      self.list_of_path = DataPreprocessing.list_csv_files(directory)\n",
    "      self.train_size = train_size\n",
    "      self.independent_signals = independent_signals\n",
    "      self.dependent_signals = dependent_signals\n",
    "      self.sequence_length = sequence_length\n",
    "\n",
    "      self.train_df_batch = []\n",
    "      self.test_df_batch = []\n",
    "      self.dataset_batch = []\n",
    "      self.train_sequences = []\n",
    "      self.test_sequences = []\n",
    "\n",
    "      self.scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "    \n",
    "    @staticmethod\n",
    "    def split_dataset(data, train_size):\n",
    "      train_size = int(len(data)* train_size)\n",
    "      train_df, test_df = data[:train_size], data[train_size + 1:]\n",
    "      return train_df, test_df  \n",
    "\n",
    "    @staticmethod\n",
    "    def list_csv_files(directory):\n",
    "      csv_files = []\n",
    "      for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".csv\"):\n",
    "          csv_files.append(filename)\n",
    "      return csv_files    \n",
    "    \n",
    "    def scaling_fit(self):\n",
    "      self.scaler.fit(pd.concat(self.train_df_batch))\n",
    "\n",
    "    def scaling_transform(self, dataset):\n",
    "      return pd.DataFrame(self.scaler.transform(dataset),\n",
    "                          index=dataset.index,\n",
    "                          columns=dataset.columns)\n",
    "    \n",
    "    def data_loader(self, file_path):\n",
    "\n",
    "      path = '{}{}'.format(self.directory, file_path) \n",
    "      print(path)\n",
    "      dataset = pd.read_csv(path)\n",
    "      dataset.timestamp = pd.to_datetime(dataset.timestamp)\n",
    "      dataset = dataset.set_index('timestamp')\n",
    "      return dataset\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def load_data_from_a_specific_path(file_path):\n",
    "      dataset = pd.read_csv(file_path)\n",
    "      dataset.timestamp = pd.to_datetime(dataset.timestamp)\n",
    "      dataset = dataset.set_index('timestamp')\n",
    "      return dataset\n",
    "    \n",
    "\n",
    "    def fit_transform(self):  \n",
    "      for each_file_path in self.list_of_path:\n",
    "        # Loading data from each file\n",
    "        dataset = self.data_loader(each_file_path)\n",
    "\n",
    "        # Calculate min and max value of all features this will be used for scaling\n",
    "        self.dataset_batch.append(dataset)\n",
    "        \n",
    "        train_df, test_df = DataPreprocessing.split_dataset(dataset, self.train_size)\n",
    "        self.train_df_batch.append(train_df)\n",
    "        self.test_df_batch.append(test_df)\n",
    "\n",
    "      # Scaling\n",
    "      self.scaling_fit()\n",
    "      \n",
    "      # Scaling transform\n",
    "      for batch_idx in range(len(self.train_df_batch)):\n",
    "        self.train_df_batch[batch_idx] = (self.scaling_transform(self.train_df_batch[batch_idx]))\n",
    "        self.test_df_batch[batch_idx] = self.scaling_transform(self.test_df_batch[batch_idx])\n",
    "\n",
    "\n",
    "      # For train data\n",
    "      self.create_sequences(self.sequence_length)\n",
    "      # For test data\n",
    "      self.create_sequences(self.sequence_length, get_sequences_for_train = False)\n",
    "\n",
    "    def transform(self, path):\n",
    "        dataset = DataPreprocessing.load_data_from_a_specific_path(path)\n",
    "        self.datset_sequences = []\n",
    "        scaled_dataset = (self.scaling_transform(dataset))\n",
    "        self.create_sequences_for_a_batch(scaled_dataset, self.sequence_length, None)\n",
    "        return self.datset_sequences\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def create_sequences_for_a_batch(self, input_data, sequence_length, get_sequences_for_train):\n",
    "\n",
    "      data_size = len(input_data)\n",
    "\n",
    "      for i in range(data_size - sequence_length):\n",
    "\n",
    "        sequence = input_data[i: i+sequence_length][self.independent_signals]\n",
    "\n",
    "        if get_sequences_for_train: \n",
    "          label = input_data[i: i + sequence_length][self.dependent_signals]\n",
    "          self.train_sequences.append((sequence, label))\n",
    "\n",
    "        elif get_sequences_for_train ==None:\n",
    "          self.datset_sequences.append((sequence, None)) \n",
    "\n",
    "        else:\n",
    "          label = input_data[i: i + sequence_length][self.dependent_signals]\n",
    "          self.test_sequences.append((sequence,label))  \n",
    "\n",
    "\n",
    "    def create_sequences(self, sequence_length, get_sequences_for_train=True):\n",
    "\n",
    "      if get_sequences_for_train:\n",
    "\n",
    "        for each_batch in self.train_df_batch:\n",
    "          self.create_sequences_for_a_batch(each_batch, sequence_length, get_sequences_for_train)\n",
    "\n",
    "      else:\n",
    "\n",
    "        for each_batch in self.test_df_batch:\n",
    "          self.create_sequences_for_a_batch(each_batch, sequence_length, get_sequences_for_train)  \n",
    "        \n",
    "\n",
    "    # Getter method\n",
    "\n",
    "    @property\n",
    "    def get_train_df(self):\n",
    "      return self.train_df_batch\n",
    "\n",
    "    @property\n",
    "    def get_test_df(self):\n",
    "      return self.test_df_batch \n",
    "    \n",
    "    @property\n",
    "    def get_sequences_train(self):\n",
    "      return self.train_sequences\n",
    "    \n",
    "    @property\n",
    "    def get_sequences_test(self):\n",
    "      return self.test_sequences\n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj1 = DataPreprocessing('datasets/', independent_signals, dependent_signals, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/resampled_20180810150607.csv\n",
      "datasets/resampled_20190401121727.csv\n",
      "datasets/resampled_20190401145936.csv\n"
     ]
    }
   ],
   "source": [
    "obj1.fit_transform()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
