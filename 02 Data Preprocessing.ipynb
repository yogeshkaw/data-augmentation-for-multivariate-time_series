{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def list_csv_files(directory):\\n    csv_files = []\\n    for filename in os.listdir(directory):\\n        if filename.endswith(\".csv\"):\\n            csv_files.append(filename)\\n    return csv_files\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "tqdm.pandas() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_data = ['latitude_degree','longitude_degree']\n",
    "\n",
    "independent_signals = ['accelerator_pedal','accelerator_pedal_gradient_sign','brake_pressure','steering_angle_calculated','steering_angle_calculated_sign']\n",
    "dependent_signals = ['vehicle_speed','roll_angle','pitch_angle']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spliting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessing():\n",
    "    def __init__(self, directory, train_size=.9, do_train_test_split = True):\n",
    "        \n",
    "      self.directory = directory\n",
    "      self.do_train_test_split = do_train_test_split\n",
    "      self.list_of_path = DataPreprocessing.list_csv_files(directory)\n",
    "      self.train_size = train_size\n",
    "      self.train_df_batch = []\n",
    "      self.test_df_batch = []\n",
    "      self.dataset_batch = []\n",
    "      self.train_sequences = []\n",
    "      self.test_sequences = []\n",
    "      self.scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "    \n",
    "    @staticmethod\n",
    "    def split_dataset(data, train_size):\n",
    "      train_size = int(len(data)* train_size)\n",
    "      train_df, test_df = data[:train_size], data[train_size + 1:]\n",
    "      return train_df, test_df  \n",
    "\n",
    "    @staticmethod\n",
    "    def list_csv_files(directory):\n",
    "      csv_files = []\n",
    "      for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".csv\"):\n",
    "          csv_files.append(filename)\n",
    "      return csv_files    \n",
    "    \n",
    "    def scaling_fit(self):\n",
    "      self.scaler.fit(pd.concat(self.train_df_batch))\n",
    "\n",
    "    def scaling_transform(self, dataset):\n",
    "      return pd.DataFrame(self.scaler.transform(dataset),\n",
    "                          index=dataset.index,\n",
    "                          columns=dataset.columns)\n",
    "    \n",
    "    def data_loader(self, file_path):\n",
    "\n",
    "      path = '{}{}'.format(self.directory, file_path)\n",
    "      print(path)\n",
    "      dataset = pd.read_csv(path)\n",
    "      dataset.timestamp = pd.to_datetime(dataset.timestamp)\n",
    "      dataset = dataset.set_index('timestamp')\n",
    "      return dataset\n",
    "\n",
    "    def fit(self): \n",
    "      for each_file_path in self.list_of_path:\n",
    "        # Loading data from each file\n",
    "        dataset = self.data_loader(each_file_path)\n",
    "\n",
    "        # Calculate min and max value of all features this will be used for scaling\n",
    "        self.dataset_batch.append(dataset)\n",
    "        if self.do_train_test_split:\n",
    "          train_df, test_df = DataPreprocessing.split_dataset(dataset, self.train_size)\n",
    "          self.train_df_batch.append(train_df)\n",
    "          self.test_df_batch.append(test_df)\n",
    "\n",
    "      # Scaling\n",
    "      self.scaling_fit()\n",
    "\n",
    "      # Scaling transform\n",
    "      for batch_idx in range(len(self.train_df_batch)):\n",
    "        self.train_df_batch[batch_idx] = (self.scaling_transform(self.train_df_batch[batch_idx]))\n",
    "        self.test_df_batch[batch_idx] = self.scaling_transform(self.test_df_batch[batch_idx])\n",
    "\n",
    "    def create_sequences_for_a_batch(input_data, target_colums, sequence_length): \n",
    "\n",
    "      sequences = []\n",
    "      data_size = len(input_data)\n",
    "\n",
    "      for i in range(data_size - sequence_length):\n",
    "        sequence = input_data[i: i+sequence_length]\n",
    "\n",
    "        label = input_data[i: i + sequence_length][target_colums]\n",
    "\n",
    "        sequences.append((sequence, label))\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "    # Getter method\n",
    "    @property\n",
    "    def get_dataset(self):\n",
    "      return self.dataset   \n",
    "\n",
    "    @property\n",
    "    def get_train_df(self):\n",
    "      return self.train_df_batch\n",
    "\n",
    "    @property\n",
    "    def get_test_df(self):\n",
    "      return self.test_df_batch \n",
    "    \n",
    "\n",
    "      \n",
    "     # TODO create sequence method\n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj1 = DataPreprocessing('datasets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/resampled_20180810150607.csv\n",
      "datasets/resampled_20190401121727.csv\n",
      "datasets/resampled_20190401145936.csv\n"
     ]
    }
   ],
   "source": [
    "obj1.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
